# Thinking1	逻辑回归的假设条件是怎样的？ #
答：
1、target应该服从伯努利分布，即0-1分布。
2.正类的概率由sigmoid函数计算，既输出的结果为样本为正的概率，假设为p，样本为负的概率为1-p

# Thinking2	逻辑回归的损失函数是怎样的？ #
答：在逻辑回归中，最常用的是代价函数是交叉熵。
交叉熵是用来衡量两个概率分布之间的差异。交叉熵越大，两个分布之间的差异越大，越对实验结果感到意外，反之，交叉熵越小，两个分布越相似，越符合预期
# Thinking3	逻辑回归如何进行分类？ #
答：逻辑回归最终得出的其实是样本等于1的概率值，所以要是做分类的话，可以设置一个阈值，大于这个阈值表示为1，小于则为0，实现分类结果。
# Thinking4	为什么在训练中需要将高度相关的特征去掉？ #
答：其实在做特征工程的时候，利用pearson系数法，去掉相关性为0.8以上，即相关性高的特征是很重要的一个步骤。因为如果两个特征之间的相关性高，则表示一个特征可以利用线性组合的方式去达到另一个特征的。一是不必要进行保留，因为2个特征表示的信息其实可以由一个特征来表达。二是减少特征的数量，可以减少模型训练的时间。其次，将相关性高的特征去掉的话，最终得出的结果可解释性也比较高。


